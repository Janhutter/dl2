[25/05/23 13:57:49] [utils.py:   82]: PyTorch Version: torch=2.5.0+cu124, cuda=12.4, cudnn=90100
[25/05/23 13:57:49] [utils.py:   83]: BN:
  EPS: 1e-05
  MOM: 0.1
CKPT_DIR: ./ckpt
CORRUPTION:
  DATASET: cifar10
  IMG_SIZE: 32
  NUM_CHANNEL: 3
  NUM_CLASSES: 10
  NUM_EX: 10000
  SEVERITY: [5, 4, 3, 2, 1]
  TYPE: ['gaussian_noise', 'shot_noise', 'impulse_noise', 'defocus_blur', 'glass_blur', 'motion_blur', 'zoom_blur', 'snow', 'frost', 'fog', 'brightness', 'contrast', 'elastic_transform', 'pixelate', 'jpeg_compression']
CUDNN:
  BENCHMARK: True
DATA_DIR: /scratch-shared/tea2/cifar10
DESC: 
EARLY_STOP_BEGIN: 70
EARLY_STOP_PATIENCE: 30
EATA:
  D_MARGIN: 0.05
  E_MARGIN: 2.763102111592855
  FISHER_ALPHA: 2000.0
  FISHER_SIZE: 2000
  USE_FISHER: False
EBM:
  BUFFER_SIZE: 10000
  REINIT_FREQ: 0.05
  SGLD_LR: 0.1
  SGLD_STD: 0.01
  STEPS: 40
  UNCOND: cond
LOG_DEST: energy_visz_test_pt_bn_adam-2-0.0001-512_cond-40-0.1-0.01-10000-0.05_250523-135749.txt
LOG_TIME: 250523-135749
MODEL:
  ADAPTATION: energy
  ADA_PARAM: ['bn']
  ARCH: WRN2810_BN
  CHECKPOINT_PTH: ./ckpt/cifar10/WRN2810_TET/TET_epoch_200.pth
  EPISODIC: False
OPTIM:
  BATCH_SIZE: 512
  BETA: 0.9
  CLIP_NORM: False
  DAMPENING: 0.0
  LAMBDA_CLS: 1.0
  LAMBDA_ENERGY: 1.0
  LR: 0.0001
  METHOD: Adam
  MOMENTUM: 0.9
  NESTEROV: True
  N_EPOCHS: 1
  SCHEDULER_GAMMA: 0.2
  SCHEDULER_MILESTONES: [60, 120, 160]
  STEPS: 2
  TEST_BATCH_SIZE: 128
  WARMUP_START_LR: 1e-07
  WARMUP_STEPS: 60
  WD: 0.0
OPTIM_ENERGY:
  BATCH_SIZE: 128
  BETA: 0.9
  CLIP_NORM: False
  DAMPENING: 0.0
  LR: 0.001
  METHOD: Adam
  MOMENTUM: 0.9
  NESTEROV: True
  STEPS: 1
  WD: 0.0
PL:
  ALPHA: 0.1
  THRESHOLD: 0.9
RNG_SEED: 1
SAR:
  MARGIN_E0: 2.763102111592855
SAVE_DIR: ./save/cifar10/bn-wrn-28-10
SHOT:
  CLF_COEFF: 0.1
  THRESHOLD: 0.9
TEST:
  
wandb: Currently logged in as: janhutter (jan-hutter) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.11
wandb: Run data is saved locally in /gpfs/home5/jhutter/dl2/wandb/run-20250523_135750-afgq06xq
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run visz-pt-model
wandb: ‚≠êÔ∏è View project at https://wandb.ai/jan-hutter/TET
wandb: üöÄ View run at https://wandb.ai/jan-hutter/TET/runs/afgq06xq
[25/05/23 13:57:51] [main_visz_pt.py:   58]: loading model checkpoint from ./ckpt/cifar10/WRN2810_TET/TET_epoch_200.pth
[25/05/23 13:57:52] [main_visz_pt.py:   99]: test-time adaptation: ENERGY
[25/05/23 13:57:52] [param.py:   18]: adapting weights of batch-normalization layer
[25/05/23 13:57:52] [optim.py:   86]: Warmup scheduler configured: 60 steps, start_lr: 1e-07, target_lr: 0.0001
[25/05/23 13:57:52] [setada.py:  167]: model for adaptation: WideResNet(
  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
  (block1): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(16, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(16, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(160, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block2): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 320, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(160, 320, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(320, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (block3): NetworkBlock(
    (layer): Sequential(
      (0): BasicBlock(
        (bn1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 640, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (convShortcut): Conv2d(320, 640, kernel_size=(1, 1), stride=(2, 2), bias=False)
      )
      (1): BasicBlock(
        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (2): BasicBlock(
        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (3): BasicBlock(
        (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(640, 640, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
  )
  (bn1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)
  (relu): ReLU(inplace=True)
  (fc): Linear(in_features=640, out_features=10, bias=True)
)
[25/05/23 13:57:52] [setada.py:  168]: params for adaptation: ['block1.layer.0.bn1.weight', 'block1.layer.0.bn1.bias', 'block1.layer.0.bn2.weight', 'block1.layer.0.bn2.bias', 'block1.layer.1.bn1.weight', 'block1.layer.1.bn1.bias', 'block1.layer.1.bn2.weight', 'block1.layer.1.bn2.bias', 'block1.layer.2.bn1.weight', 'block1.layer.2.bn1.bias', 'block1.layer.2.bn2.weight', 'block1.layer.2.bn2.bias', 'block1.layer.3.bn1.weight', 'block1.layer.3.bn1.bias', 'block1.layer.3.bn2.weight', 'block1.layer.3.bn2.bias', 'block2.layer.0.bn1.weight', 'block2.layer.0.bn1.bias', 'block2.layer.0.bn2.weight', 'block2.layer.0.bn2.bias', 'block2.layer.1.bn1.weight', 'block2.layer.1.bn1.bias', 'block2.layer.1.bn2.weight', 'block2.layer.1.bn2.bias', 'block2.layer.2.bn1.weight', 'block2.layer.2.bn1.bias', 'block2.layer.2.bn2.weight', 'block2.layer.2.bn2.bias', 'block2.layer.3.bn1.weight', 'block2.layer.3.bn1.bias', 'block2.layer.3.bn2.weight', 'block2.layer.3.bn2.bias', 'block3.layer.0.bn1.weight', 'block3.layer.0.bn1.bias', 'block3.layer.0.bn2.weight', 'block3.layer.0.bn2.bias', 'block3.layer.1.bn1.weight', 'block3.layer.1.bn1.bias', 'block3.layer.1.bn2.weight', 'block3.layer.1.bn2.bias', 'block3.layer.2.bn1.weight', 'block3.layer.2.bn1.bias', 'block3.layer.2.bn2.weight', 'block3.layer.2.bn2.bias', 'block3.layer.3.bn1.weight', 'block3.layer.3.bn1.bias', 'block3.layer.3.bn2.weight', 'block3.layer.3.bn2.bias', 'bn1.weight', 'bn1.bias']
[25/05/23 13:57:52] [setada.py:  169]: optimizer for adaptation: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0001
    maximize: False
    weight_decay: 0.0
)
[25/05/23 13:57:52] [eval_visz.py:  115]: not resetting model
[25/05/23 13:58:01] [eval_visz.py:   49]: batch_counter=0
[25/05/23 13:58:10] [optim.py:   59]: Warmup step 1/60, LR: 0.000002
[25/05/23 13:58:25] [eval_visz.py:   65]: batch_acc: 0.927734375
[25/05/23 13:58:25] [eval_visz.py:   49]: batch_counter=1
[25/05/23 13:58:32] [optim.py:   59]: Warmup step 2/60, LR: 0.000003
[25/05/23 13:58:47] [eval_visz.py:   65]: batch_acc: 0.9453125
[25/05/23 13:58:47] [eval_visz.py:   49]: batch_counter=2
[25/05/23 13:58:55] [optim.py:   59]: Warmup step 3/60, LR: 0.000005
[25/05/23 13:59:10] [eval_visz.py:   65]: batch_acc: 0.939453125
[25/05/23 13:59:10] [eval_visz.py:   49]: batch_counter=3
[25/05/23 13:59:18] [optim.py:   59]: Warmup step 4/60, LR: 0.000007
[25/05/23 13:59:33] [eval_visz.py:   65]: batch_acc: 0.94921875
[25/05/23 13:59:33] [eval_visz.py:   49]: batch_counter=4
[25/05/23 13:59:41] [optim.py:   59]: Warmup step 5/60, LR: 0.000008
[25/05/23 13:59:56] [eval_visz.py:   65]: batch_acc: 0.923828125
[25/05/23 13:59:56] [eval_visz.py:   49]: batch_counter=5
[25/05/23 14:00:04] [optim.py:   59]: Warmup step 6/60, LR: 0.000010
[25/05/23 14:00:20] [eval_visz.py:   65]: batch_acc: 0.927734375
[25/05/23 14:00:20] [eval_visz.py:   49]: batch_counter=6
[25/05/23 14:00:27] [optim.py:   59]: Warmup step 7/60, LR: 0.000012
[25/05/23 14:00:43] [eval_visz.py:   65]: batch_acc: 0.947265625
[25/05/23 14:00:43] [eval_visz.py:   49]: batch_counter=7
[25/05/23 14:00:51] [optim.py:   59]: Warmup step 8/60, LR: 0.000013
[25/05/23 14:01:07] [eval_visz.py:   65]: batch_acc: 0.919921875
[25/05/23 14:01:07] [eval_visz.py:   49]: batch_counter=8
[25/05/23 14:01:14] [optim.py:   59]: Warmup step 9/60, LR: 0.000015
[25/05/23 14:01:31] [eval_visz.py:   65]: batch_acc: 0.9296875
[25/05/23 14:01:31] [eval_visz.py:   49]: batch_counter=9
[25/05/23 14:01:38] [optim.py:   59]: Warmup step 10/60, LR: 0.000017
[25/05/23 14:01:55] [eval_visz.py:   65]: batch_acc: 0.923828125
[25/05/23 14:01:55] [eval_visz.py:   49]: batch_counter=10
[25/05/23 14:02:03] [optim.py:   59]: Warmup step 11/60, LR: 0.000018
[25/05/23 14:02:20] [eval_visz.py:   65]: batch_acc: 0.935546875
[25/05/23 14:02:20] [eval_visz.py:   49]: batch_counter=11
[25/05/23 14:02:27] [optim.py:   59]: Warmup step 12/60, LR: 0.000020
[25/05/23 14:02:44] [eval_visz.py:   65]: batch_acc: 0.931640625
[25/05/23 14:02:44] [eval_visz.py:   49]: batch_counter=12
[25/05/23 14:02:52] [optim.py:   59]: Warmup step 13/60, LR: 0.000022
[25/05/23 14:03:09] [eval_visz.py:   65]: batch_acc: 0.91796875
[25/05/23 14:03:09] [eval_visz.py:   49]: batch_counter=13
[25/05/23 14:03:17] [optim.py:   59]: Warmup step 14/60, LR: 0.000023
[25/05/23 14:03:34] [eval_visz.py:   65]: batch_acc: 0.935546875
[25/05/23 14:03:34] [eval_visz.py:   49]: batch_counter=14
[25/05/23 14:03:42] [optim.py:   59]: Warmup step 15/60, LR: 0.000025
[25/05/23 14:03:59] [eval_visz.py:   65]: batch_acc: 0.943359375
[25/05/23 14:03:59] [eval_visz.py:   49]: batch_counter=15
[25/05/23 14:04:07] [optim.py:   59]: Warmup step 16/60, LR: 0.000027
[25/05/23 14:04:25] [eval_visz.py:   65]: batch_acc: 0.951171875
[25/05/23 14:04:25] [eval_visz.py:   49]: batch_counter=16
[25/05/23 14:04:32] [optim.py:   59]: Warmup step 17/60, LR: 0.000028
[25/05/23 14:04:50] [eval_visz.py:   65]: batch_acc: 0.92578125
[25/05/23 14:04:50] [eval_visz.py:   49]: batch_counter=17
[25/05/23 14:04:58] [optim.py:   59]: Warmup step 18/60, LR: 0.000030
[25/05/23 14:05:16] [eval_visz.py:   65]: batch_acc: 0.92578125
[25/05/23 14:05:16] [eval_visz.py:   49]: batch_counter=18
[25/05/23 14:05:23] [optim.py:   59]: Warmup step 19/60, LR: 0.000032
[25/05/23 14:05:41] [eval_visz.py:   65]: batch_acc: 0.927734375
[25/05/23 14:05:41] [eval_visz.py:   49]: batch_counter=19
[25/05/23 14:05:46] [optim.py:   59]: Warmup step 20/60, LR: 0.000033
[25/05/23 14:06:05] [eval_visz.py:   65]: batch_acc: 0.9485294222831726
[25/05/23 14:06:05] [eval_visz.py:  120]: acc: 93.39%
Building model...
Files already downloaded and verified
torch.Size([10000, 3, 32, 32]) 10000
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mvisz-pt-model[0m at: [34mhttps://wandb.ai/jan-hutter/TET/runs/afgq06xq[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250523_135750-afgq06xq/logs[0m
